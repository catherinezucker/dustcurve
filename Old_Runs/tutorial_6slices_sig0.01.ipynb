{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics\n",
    "\n",
    "We are going to use parallel-tempering, implemented via the python emcee package, to explore our posterior, which consists of the set of distances and gas to dust conversion coefficients to the six velocity slices towards the center of the Cepheus molecular cloud. Since we need to explore a 12 dimensional parameter space, we are going to use 50 walkers, 10000 steps each, at 5 different temperatures. If you would like to edit this parameters, simply edit \"nwalkers\", \"ntemps\", and \"nsteps\" in the cell below. However, we are only going to keep the lowest temperature chain ($\\beta=1$) for analysis. Since the sampler.chain object from PTSampler returns an array of shape (Ntemps, Nwalkers, Nsteps, Ndim), returning the samples for all walkers, steps, and dimensions at $\\beta=1$ would correspond to sampler.chain[0,:,:,:]. To decrease your value of $\\beta$ simply increase the index for the first dimension. For more information on how PTSampler works, see http://dan.iel.fm/emcee/current/user/pt/. We will set off our walkers in a Gaussian ball around a) the kinematic distance estimates for the Cepheus molecular cloud given by a flat rotation curve from Leroy & Rosolowsky 2006 and b) the gas-to-dust coefficient given by the literature. We perturb the walkers in a Gaussian ball with mean 0 and variance 1. You can edit the starting positions of the walkers by editing the \"result\" variable below. We are going to discard the first half of every walker's chain as burn-in. \n",
    "\n",
    "### Setting up the positional arguments for PTSampler\n",
    "\n",
    "We need to feed PTSampler the required positional arguments for the log_likelihood and log_prior function. We do this using the fetch_args function from the io module, which creates an instance of the pixclass object that holds our data and metadata. Fetch_args accepts three arguments: \n",
    "\n",
    "- A string specifiying the h5 filenames containing your data, in our case 10 healpix nside 128 pixels centered around (l,b)=(109.75, 13.75), which covers a total area of 2 sq. deg. \n",
    "- The prior bounds you want to impose on distances (flat prior) and the standard deviation you'd like for the log-normal prior on the conversion coefficients. For distances, this must be between 4 and 19, because that's the distance modulus range of our stellar posterior array. The prior bounds must be in the format [lowerbound_distance, upperbound_distance, sigma]\n",
    "- The gas-to-dust coefficient you'd like to use, given as a float; for this tutorial, we are pulling a value from the literature of 0.06 magnitudes/K. This value is then multiplied by the set of c coefficients we're determining as part of the parameter estimation problem. \n",
    "\n",
    "Fetch_args will then return the correct arguments for the log_likelihood and log_prior functions within the model module. \n",
    "\n",
    "Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "from dustcurve import model\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from dustcurve import pixclass\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from dustcurve import io\n",
    "from dustcurve import hputils\n",
    "from dustcurve import kdist\n",
    "%matplotlib inline\n",
    "\n",
    "#suppress obnoxious deprecation warning that doesn't affect output\n",
    "warnings.filterwarnings(\"ignore\", category=Warning, module=\"emcee\")\n",
    "\n",
    "#our pixels of choice\n",
    "indices=hputils.list_indices(128,(109.0,110.5,13.0,14.5)).astype(str)\n",
    "fnames=[str(i)+'.h5' for i in indices]\n",
    "\n",
    "#fetch the required likelihood and prior arguments for PTSampler\n",
    "ldata,pdata=io.fetch_args(fnames,[4,19,0.001],0.06)\n",
    "\n",
    "# the model has 12 parameters; we'll use 50 walkers, 10000 steps each, at 5 different temps\n",
    "ndim=12\n",
    "nslices=6\n",
    "nwalkers = 50\n",
    "nsteps = 10000\n",
    "ntemps=5\n",
    "\n",
    "#setting off the walkers at the kinematic distance given by the literature, assuming a flat rotation curve, theta=220 km/s, R=8.5 kpc\n",
    "#Details on rotation curve given in Rosolowsky and Leroy 2006\n",
    "vslices=np.linspace(-11.7,-5.2,nslices)\n",
    "klong=np.ones(nslices)*109.75\n",
    "klat=np.ones(nslices)*13.75\n",
    "kdist=kdist.kdist(klong,klat,vslices)\n",
    "kdistmod=5*np.log10(kdist)-5\n",
    "\n",
    "#slightly perturb the starting positions for each walker, in a ball centered around result\n",
    "#perturb all walkers in a Gaussian ball with mean 0 and variance 1\n",
    "result_dist=kdistmod.tolist()\n",
    "result_coeff= [1.0 for i in range (nslices)]\n",
    "starting_positions_dist=np.array([[result_dist + np.random.randn(nslices) for i in range(nwalkers)] for j in range(ntemps)]).clip(4,19)\n",
    "starting_positions_coeff=np.array([[result_coeff + 0.5*np.random.randn(nslices) for i in range(nwalkers)] for j in range(ntemps)]).clip(0)\n",
    "starting_positions=np.concatenate((starting_positions_dist,starting_positions_coeff), axis=2)\n",
    "\n",
    "#set up the sampler object\n",
    "sampler = emcee.PTSampler(ntemps, nwalkers, ndim, model.log_likelihood, model.log_prior, loglargs=(ldata), logpargs=[pdata], threads=8)\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run the sampler and time how long it takes\n",
    "%time sampler.run_mcmc(starting_positions, nsteps) \n",
    "print('Sampler Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampler is done running, so now let's check out the results. We are going to print out our mean acceptance fraction across all walkers for the coldest temperature chain. \n",
    "\n",
    "We are also going to discard the first half of each walker's chain as burn-in; to change the number of steps to burn off, simply edit the 3rd dimension of sampler.chain[0,:,n:,:] and input your desired value of n. Next, we are going to compute and print out the 50th, 16th, and 84th percentile of the chains for each distance parameter, using the \"quantile\" attribute of a pandas dataframe object. The 50th percentile measurement represents are best guess for the each distance parameter, while the difference between the 16th and 50th gives us a lower limit and the difference between the 50th and 84th percentile gives us an upper limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract the coldest [beta=1] temperature chain from the sampler object; discard first half of samples as burnin\n",
    "samples_cold = sampler.chain[0,:,int(.5*nsteps):,:]\n",
    "traces_cold = samples_cold.reshape(-1, ndim).T\n",
    "\n",
    "\n",
    "#check out acceptance fraction:\n",
    "print(\"Our mean acceptance fraction for the coldest chain is %.2f\" % np.mean(sampler.acceptance_fraction[0]))\n",
    "\n",
    "#find best fit values for each of the 24 parameters (12 d's and 12 c's)\n",
    "theta=pd.DataFrame(traces_cold)\n",
    "quantile_50=theta.quantile(.50, axis=1).values\n",
    "quantile_84=theta.quantile(.84, axis=1).values\n",
    "quantile_16=theta.quantile(.16, axis=1).values\n",
    "\n",
    "upperlim=quantile_84-quantile_50\n",
    "lowerlim=quantile_50-quantile_16\n",
    "\n",
    "#print out distances\n",
    "for i in range(0,int(len(quantile_50)/2)):\n",
    "    print('d%i: %.3f + %.3f - %.3f' % (i+1,quantile_50[i],upperlim[i], lowerlim[i]))\n",
    "\n",
    "#print out coefficients\n",
    "for i in range(int(len(quantile_50)/2), int(len(quantile_50))):\n",
    "    print('c%i: %.3f + %.3f - %.3f' % (i+1-int(len(quantile_50)/2),quantile_50[i],upperlim[i], lowerlim[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our chains look like by producing trace plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set up subplots for chain plotting\n",
    "axes=['ax'+str(i) for i in range(ndim)]\n",
    "fig, (axes) = plt.subplots(ndim, figsize=(10,60))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(0,ndim):\n",
    "    if i<int(ndim/2):\n",
    "        axes[i].set(ylabel='d%i' % (i+1))\n",
    "    else:\n",
    "        axes[i].set(ylabel='c%i' % (i-5))\n",
    "\n",
    "#plot traces for each parameter\n",
    "for i in range(0,ndim):\n",
    "    sns.tsplot(traces_cold[i],ax=axes[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the seaborn distplot function to plot histograms of the last half of the traces for each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set up subplots for histogram plotting\n",
    "axes=['ax'+str(i) for i in range(ndim)]\n",
    "fig, (axes) = plt.subplots(ndim, figsize=(10,60))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(0,ndim):\n",
    "    if i<int(ndim/2):\n",
    "        axes[i].set(ylabel='d%i' % (i+1))\n",
    "    else:\n",
    "        axes[i].set(ylabel='c%i' % (i-5))\n",
    "\n",
    "#plot traces for each parameter\n",
    "for i in range(0,ndim):\n",
    "    sns.distplot(traces_cold[i],ax=axes[i],hist=True,norm_hist=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's overplot the reddening profiles corresponding to our most probable parameters on top of the stacked stellar posterior surfaces. We show two options. The first plot has been normalized so that a) each individual stellar posterior array sums to one and b) each distance column in the stacked posterior array contains the same amount of \"ink.\" The second plot just normalized so each individual stellar posterior array sums to 1. Each colored reddening profile corresponds to a single pixel in the Dame et al. 2001 CO spectral cube. There are stars in ~175 Dame et al. 2001 CO pixels in our region of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dustcurve import pixclass\n",
    "\n",
    "post_all=np.empty((0,700,120))\n",
    "\n",
    "for file in fnames:\n",
    "    pixObj=pixclass.PixStars('/n/fink1/czucker/Data/'+file)\n",
    "    post_array=np.asarray(pixObj.get_p()[:,:,:])\n",
    "    post_all=np.vstack((post_all,post_array))\n",
    "        \n",
    "unique_co,unique_post,ratio=ldata\n",
    "\n",
    "from dustcurve import plot_posterior\n",
    "#plot the reddening profile over the stacked, normalized stellar posterior surfaces  \n",
    "#normcol=True, normsurf=True\n",
    "plot_posterior.plot_posterior(np.asarray(post_all),np.linspace(4,19,120),np.linspace(0,7,700),quantile_50,ratio,unique_co,y_range=[0,2],vmax=0.03,normcol=True)\n",
    "plot_posterior.plot_posterior(np.asarray(post_all),np.linspace(4,19,120),np.linspace(0,7,700),quantile_50,ratio,unique_co,y_range=[0,2],vmax=10.0,normcol=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how similar the parameters at different steps are. To do this, we draw one thousand random samples from the last half of the chain and plot the reddening profile corresponding to those parameters in light blue. Then, we plot the \"best fit\" reddening profile corresponding to the 50th quantile parameters (essentially the median of the last half of the chains). We are normalizing the surfaces in two different ways, as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dustcurve import plot_posterior\n",
    "import numpy as np\n",
    "plot_posterior.plot_samples(np.asarray(post_all),np.linspace(4,19,120),np.linspace(0,7,700),quantile_50,traces_cold,ratio,unique_co,y_range=[0,2],vmax=0.03,normcol=True)\n",
    "plot_posterior.plot_samples(np.asarray(post_all),np.linspace(4,19,120),np.linspace(0,7,700),quantile_50,traces_cold,ratio,unique_co,y_range=[0,2],vmax=20,normcol=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the results to a file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "#Save the results of the sampler:\n",
    "#our pixels of choice\n",
    "output='2degrees_june21_0.01sig.h5'\n",
    "\n",
    "fwrite = h5py.File(\"/n/fink1/czucker/Output/\"+str(output), \"w\")\n",
    "chaindata = fwrite.create_dataset(\"/chains\", sampler.chain.shape, dtype='f')\n",
    "chaindata[:,:,:,:]=sampler.chain\n",
    "    \n",
    "probdata = fwrite.create_dataset(\"/probs\", sampler.lnprobability.shape, dtype='f')\n",
    "probdata[:,:,:]=sampler.lnprobability\n",
    "\n",
    "\n",
    "fwrite.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
